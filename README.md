# Statistical-Natural-Language-Processing
UCSD CSE 256 Statistical NLP course projects and assignments

| **Assignment** | **Description** |
|----------------|-----------------|
| **Project** | Performed sentiment analysis on the HappyDB dataset for emotion classification. Gradient-boosting models are effective for text classification, balancing speed and performance. LSTM and its variants offer the best accuracy, though at the cost of time. Surprisingly, regression models performed well despite their simplicity.|
| **CSE256 PA2** | Implemented a Deep Averaging Network (DAN) for sentiment classification, experimented with different configurations and pretrained embeddings. Implemented Byte Pair Encoding (BPE) for subword tokenization. Answered conceptual questions about the skip-gram word embedding algorithm. |
| **CSE256 PA2** | Implemented a transformer encoder and trained it for politician speech classification. Implemented a GPT-like transformer decoder and pre-trained it on language modeling. Experimented with transformer architecture aspects like positional encoding and attention patterns, evaluating model performance on classification accuracy and language modeling perplexity. |
| **CSE256 PA3** | Implemented bi-encoder retrieval and ColBERT re-ranking. Compared max pooling vs. mean pooling for relevance scoring and evaluated retrieval performance. |
| **CSE256 PA4** | Got insights into beam search decoding from a pretrained GPT-2 model, visualized and analyzed token probabilities with different beam sizes. Implemented a WordBlock score processor to prevent repetition and a BeamBlock score processor to prevent trigram repetition. |
